{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import MiniGCDataset\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "from dgl.data import citegrh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'news_headline' : {\n",
    "        'dataset_path': 'data/NewsHeadline_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'kaggle-news',\n",
    "        'epochs': 5,\n",
    "        'test_size': 0.5\n",
    "    },\n",
    "    \n",
    "    'semeval' : {\n",
    "        'dataset_path': 'data/SemEval_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'semeval',\n",
    "        'epochs': 10,\n",
    "        'test_size': 0.2\n",
    "    },\n",
    "    \n",
    "    'figlang' : {\n",
    "        'dataset_path': 'data/FigLang_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'figlang',\n",
    "        'epochs': 10,\n",
    "        'test_size': 0.2\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configs['semeval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            entry = {}\n",
    "            entry['sentences'] = []\n",
    "            \n",
    "            line = line.strip()\n",
    "            d = json.loads(line)\n",
    "            \n",
    "            entry['sentences'].append(d['sentence'])\n",
    "            entry['label'] = int(d['label'])\n",
    "            \n",
    "            for k in d['common_sense'].keys():\n",
    "                if k == 'xWant' or k == 'xEffect':\n",
    "                    entry['sentences'].append(d['common_sense'][k])\n",
    "            data.append(entry)\n",
    "                \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(config['dataset_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn(input_ids):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config['model_name'] + '-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = torch.load('model/distilbert-' + config['model_save_point'] + '.pb')\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from pickle dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def create_dataset_cached(dataset):\n",
    "    all_data = []\n",
    "    \n",
    "    for data, label in tqdm(dataset):\n",
    "        input_ids = []\n",
    "        \n",
    "        input_ids.append(data['sentence'])\n",
    "        \n",
    "        for s in data['support']:\n",
    "            input_ids.append(s)\n",
    "            \n",
    "        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        att_mask = torch.tensor(get_attn(input_ids))\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "\n",
    "        hidden_state = bert_model(input_ids.to(device), attention_mask=att_mask.to(device), output_hidden_states=True)['hidden_states']\n",
    "        output = hidden_state[-1][:, 0, :].detach().to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        graph = dgl.graph((torch.tensor([0, 0]), \n",
    "                               torch.tensor([1, 2])))\n",
    "        graph.ndata['x'] = torch.tensor(output)\n",
    "        all_data.append((graph, label, data['raw_sentence']))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_, validationset_ = get_pickle_file('model/trainset-' + config['model_save_point'] +'.data'), get_pickle_file('model/validationset-' + config['model_save_point'] +'.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3067 [00:00<?, ?it/s]<ipython-input-21-bf78180da400>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  graph.ndata['x'] = torch.tensor(output)\n",
      "100%|██████████| 3067/3067 [00:20<00:00, 150.40it/s]\n",
      "100%|██████████| 767/767 [00:05<00:00, 128.30it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset, validationset = create_dataset_cached(trainset_), create_dataset_cached(validationset_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels, raw_sentence = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor([labels]), raw_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(validationset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic SAGE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.node_count = 3\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.index = torch.tensor([0]).to(device)\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(dgl_conv.SAGEConv(768, n_hidden, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=None))\n",
    "        self.lin1 = nn.Linear(64, 5)\n",
    "        self.lin2 = nn.Linear(5 * self.node_count, 2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)  \n",
    "        \n",
    "        x = self.lin1(h.view(-1, self.node_count, 64))\n",
    "        x = self.lin2(x.view(-1, 5 * self.node_count))\n",
    "        \n",
    "        return F.log_softmax(x.view(-1, 2), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGEModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): SAGEConv(\n",
       "      (feat_drop): Dropout(p=0.5, inplace=False)\n",
       "      (fc_neigh): Linear(in_features=768, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (lin2): Linear(in_features=15, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'gcn'\n",
    "n_classes = 2\n",
    "in_feats = trainset[0][0].ndata['x'].shape[1]\n",
    "\n",
    "model = GraphSAGEModel(in_feats,\n",
    "                             n_hidden,\n",
    "                             n_classes,\n",
    "                             n_layers,\n",
    "                             F.relu,\n",
    "                             dropout,\n",
    "                             aggregator_type)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4\n",
    "lr = 2e-3\n",
    "neg_sample_size = 100\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(bert_model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "criteria = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch, (g, label, _) in enumerate(train_loader):\n",
    "        g = g.to(device)\n",
    "        output = model(g, g.ndata['x'])\n",
    "        loss = criteria(output, label.view(-1).to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 40 == 0:\n",
    "            print(\"Epoch {:d} |Batch {:d} | Loss {:.4f} \".format(epoch, batch, loss.item()))\n",
    "        eval_accuracy += loss.item()\n",
    "        nb_eval_steps += 1\n",
    "            \n",
    "    \n",
    "    print(\"Average Training Loss: {0:.2f}\".format(eval_accuracy/nb_eval_steps)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader):   \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch, (g, label, _) in enumerate(test_loader):\n",
    "        g = g.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(g, g.ndata['x']) \n",
    "        \n",
    "\n",
    "        logits = output.detach().cpu().numpy()\n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        \n",
    "        prediction = list(np.argmax(logits, axis=1).flatten())\n",
    "        all_predictions.extend(prediction)\n",
    "        all_labels.extend(label_ids.flatten())\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    \n",
    "    accuracy = eval_accuracy/nb_eval_steps\n",
    "    \n",
    "    matrix = confusion_matrix(all_predictions, all_labels)\n",
    "\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    fn = matrix[1][0]\n",
    "    tn = matrix[1][1]\n",
    "    \n",
    "    print(\"  Sarcastic Precision: {0:.2f}\".format(tp/ (tp + fp)))\n",
    "    print(\"  Sarcastic F1-score: {0:.2f}\".format(2*tp / (2*tp + fn + fp)))\n",
    "    print(\"  Sarcastic Recall: {0:.2f}\".format(tp / (tp + fn))) \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"  Non-sarcastic Precision: {0:.2f}\".format(tn / (tn + fn)))\n",
    "    print(\"  Non-Sarcastic F1-score: {0:.2f}\".format(2*tn / (2*tn + fn + fp)))\n",
    "    print(\"  Non-sarcasm Recall: {0:.2f}\".format(tn / (tn + fp)))\n",
    "\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner():\n",
    "    best_model = model\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, config['epochs']))\n",
    "        print('Training...')\n",
    "        train(epoch)\n",
    "        acc = test(test_loader)\n",
    "        if acc > best_acc:\n",
    "            print('Saving ...')\n",
    "            best_acc = acc\n",
    "            best_model = model\n",
    "            print('Done!')\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    return best_model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "Epoch 0 |Batch 0 | Loss 0.8374 \n",
      "Epoch 0 |Batch 40 | Loss 0.6957 \n",
      "Epoch 0 |Batch 80 | Loss 0.6677 \n",
      "Epoch 0 |Batch 120 | Loss 0.7447 \n",
      "Epoch 0 |Batch 160 | Loss 0.6647 \n",
      "Average Training Loss: 0.70\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.71\n",
      "  Sarcastic F1-score: 0.43\n",
      "  Sarcastic Recall: 0.31\n",
      "\n",
      "  Non-sarcastic Precision: 0.56\n",
      "  Non-Sarcastic F1-score: 0.68\n",
      "  Non-sarcasm Recall: 0.87\n",
      "Saving ...\n",
      "Done!\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "Epoch 1 |Batch 0 | Loss 0.7113 \n",
      "Epoch 1 |Batch 40 | Loss 0.6775 \n",
      "Epoch 1 |Batch 80 | Loss 0.7139 \n",
      "Epoch 1 |Batch 120 | Loss 0.7215 \n",
      "Epoch 1 |Batch 160 | Loss 0.5822 \n",
      "Average Training Loss: 0.67\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.56\n",
      "  Sarcastic F1-score: 0.58\n",
      "  Sarcastic Recall: 0.61\n",
      "\n",
      "  Non-sarcastic Precision: 0.57\n",
      "  Non-Sarcastic F1-score: 0.55\n",
      "  Non-sarcasm Recall: 0.53\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "Epoch 2 |Batch 0 | Loss 0.6270 \n",
      "Epoch 2 |Batch 40 | Loss 0.6582 \n",
      "Epoch 2 |Batch 80 | Loss 0.6180 \n",
      "Epoch 2 |Batch 120 | Loss 0.5837 \n",
      "Epoch 2 |Batch 160 | Loss 0.5532 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.59\n",
      "  Sarcastic F1-score: 0.60\n",
      "  Sarcastic Recall: 0.61\n",
      "\n",
      "  Non-sarcastic Precision: 0.59\n",
      "  Non-Sarcastic F1-score: 0.58\n",
      "  Non-sarcasm Recall: 0.57\n",
      "Saving ...\n",
      "Done!\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "Epoch 3 |Batch 0 | Loss 0.6660 \n",
      "Epoch 3 |Batch 40 | Loss 0.6813 \n",
      "Epoch 3 |Batch 80 | Loss 0.6662 \n",
      "Epoch 3 |Batch 120 | Loss 0.6010 \n",
      "Epoch 3 |Batch 160 | Loss 0.5898 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.53\n",
      "  Sarcastic F1-score: 0.67\n",
      "  Sarcastic Recall: 0.92\n",
      "\n",
      "  Non-sarcastic Precision: 0.69\n",
      "  Non-Sarcastic F1-score: 0.27\n",
      "  Non-sarcasm Recall: 0.17\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "Epoch 4 |Batch 0 | Loss 0.6561 \n",
      "Epoch 4 |Batch 40 | Loss 0.5726 \n",
      "Epoch 4 |Batch 80 | Loss 0.6666 \n",
      "Epoch 4 |Batch 120 | Loss 0.6986 \n",
      "Epoch 4 |Batch 160 | Loss 0.5592 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.66\n",
      "  Sarcastic F1-score: 0.50\n",
      "  Sarcastic Recall: 0.40\n",
      "\n",
      "  Non-sarcastic Precision: 0.57\n",
      "  Non-Sarcastic F1-score: 0.66\n",
      "  Non-sarcasm Recall: 0.80\n",
      "Saving ...\n",
      "Done!\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "Epoch 5 |Batch 0 | Loss 0.6853 \n",
      "Epoch 5 |Batch 40 | Loss 0.6708 \n",
      "Epoch 5 |Batch 80 | Loss 0.7035 \n",
      "Epoch 5 |Batch 120 | Loss 0.5746 \n",
      "Epoch 5 |Batch 160 | Loss 0.7618 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.54\n",
      "  Sarcastic F1-score: 0.65\n",
      "  Sarcastic Recall: 0.82\n",
      "\n",
      "  Non-sarcastic Precision: 0.63\n",
      "  Non-Sarcastic F1-score: 0.41\n",
      "  Non-sarcasm Recall: 0.30\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "Epoch 6 |Batch 0 | Loss 0.6125 \n",
      "Epoch 6 |Batch 40 | Loss 0.5745 \n",
      "Epoch 6 |Batch 80 | Loss 0.6725 \n",
      "Epoch 6 |Batch 120 | Loss 0.6603 \n",
      "Epoch 6 |Batch 160 | Loss 0.6690 \n",
      "Average Training Loss: 0.65\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.54\n",
      "  Sarcastic F1-score: 0.65\n",
      "  Sarcastic Recall: 0.80\n",
      "\n",
      "  Non-sarcastic Precision: 0.62\n",
      "  Non-Sarcastic F1-score: 0.43\n",
      "  Non-sarcasm Recall: 0.33\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "Epoch 7 |Batch 0 | Loss 0.6935 \n",
      "Epoch 7 |Batch 40 | Loss 0.5885 \n",
      "Epoch 7 |Batch 80 | Loss 0.5871 \n",
      "Epoch 7 |Batch 120 | Loss 0.6483 \n",
      "Epoch 7 |Batch 160 | Loss 0.6261 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.56\n",
      "  Sarcastic F1-score: 0.60\n",
      "  Sarcastic Recall: 0.65\n",
      "\n",
      "  Non-sarcastic Precision: 0.59\n",
      "  Non-Sarcastic F1-score: 0.54\n",
      "  Non-sarcasm Recall: 0.50\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "Epoch 8 |Batch 0 | Loss 0.6633 \n",
      "Epoch 8 |Batch 40 | Loss 0.7024 \n",
      "Epoch 8 |Batch 80 | Loss 0.5915 \n",
      "Epoch 8 |Batch 120 | Loss 0.7203 \n",
      "Epoch 8 |Batch 160 | Loss 0.6940 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.58\n",
      "  Sarcastic F1-score: 0.61\n",
      "  Sarcastic Recall: 0.63\n",
      "\n",
      "  Non-sarcastic Precision: 0.59\n",
      "  Non-Sarcastic F1-score: 0.57\n",
      "  Non-sarcasm Recall: 0.54\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "Epoch 9 |Batch 0 | Loss 0.6999 \n",
      "Epoch 9 |Batch 40 | Loss 0.7011 \n",
      "Epoch 9 |Batch 80 | Loss 0.6185 \n",
      "Epoch 9 |Batch 120 | Loss 0.5581 \n",
      "Epoch 9 |Batch 160 | Loss 0.7342 \n",
      "Average Training Loss: 0.66\n",
      "\n",
      "Running Validation...\n",
      "  Sarcastic Precision: 0.56\n",
      "  Sarcastic F1-score: 0.62\n",
      "  Sarcastic Recall: 0.70\n",
      "\n",
      "  Non-sarcastic Precision: 0.60\n",
      "  Non-Sarcastic F1-score: 0.52\n",
      "  Non-sarcasm Recall: 0.45\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GraphSAGEModel(\n",
       "   (layers): ModuleList(\n",
       "     (0): SAGEConv(\n",
       "       (feat_drop): Dropout(p=0.5, inplace=False)\n",
       "       (fc_neigh): Linear(in_features=768, out_features=64, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (lin1): Linear(in_features=64, out_features=5, bias=True)\n",
       "   (lin2): Linear(in_features=15, out_features=2, bias=True)\n",
       " ),\n",
       " 0.5973090277777778)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

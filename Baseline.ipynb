{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from localutils import *\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer, RobertaTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaForSequenceClassification, BertForSequenceClassification, AdamW, BertConfig, DistilBertForSequenceClassification, DistilBertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'news_headline' : {\n",
    "        'dataset_path': 'data/NewsHeadline_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'kaggle-news',\n",
    "        'epochs': 5,\n",
    "        'test_size': 0.5\n",
    "    },\n",
    "    \n",
    "    'semeval' : {\n",
    "        'dataset_path': 'data/SemEval_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'semeval',\n",
    "        'epochs': 10,\n",
    "        'test_size': 0.2\n",
    "    },\n",
    "    \n",
    "    'figlang' : {\n",
    "        'dataset_path': 'data/FigLang_comet_autocomplete.jsonl',\n",
    "        'model_name': 'distilbert',\n",
    "        'model_save_point': 'figlang',\n",
    "        'epochs': 10,\n",
    "        'test_size': 0.2\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            entry = {}\n",
    "            \n",
    "            line = line.strip()\n",
    "            d = json.loads(line)\n",
    "            \n",
    "            entry['sentence'] = d['sentence']\n",
    "            entry['label'] = int(d['label'])\n",
    "            entry['support'] = []\n",
    "\n",
    "            for k in d['common_sense'].keys():\n",
    "                if k == 'xWant' or k == 'xEffect':\n",
    "                    if dataset_name == 'semeval' or dataset_name == 'figlang':\n",
    "                        entry['support'].append(d['common_sense'][k])\n",
    "                    elif dataset_name == 'news_headline':\n",
    "                        entry['support'].append(d['common_sense'][k][0] if d['common_sense'][k][0] != 'none' else d['common_sense'][k][1])\n",
    "            dataset.append(entry)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'news_headline'\n",
    "config = configs[dataset_name]\n",
    "\n",
    "PATH = config['dataset_path']\n",
    "\n",
    "dataset = load_dataset(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "if model_name == \"distilbert\":\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"roberta\":\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all(sentences):\n",
    "    input_ids = []\n",
    "    for data in sentences:\n",
    "        input_ids.append(tokenizer.encode(data, add_special_tokens=True))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn(input_ids):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset):\n",
    "    all_data = []\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        input_ids = []\n",
    "        \n",
    "        input_ids.append(tokenizer.encode(data['sentence'].lower()))\n",
    "        \n",
    "        for s in data['support']:\n",
    "            input_ids.append(tokenizer.encode(s.lower()))\n",
    "            \n",
    "        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        attn_mask = torch.tensor(get_attn(input_ids))\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "        entry = {}\n",
    "        entry['raw_sentence'] = data['sentence'] + \" [SUPPORT]: \" + \" [SEP] \".join(data['support'])\n",
    "        entry['sentence'] = input_ids[0]\n",
    "        entry['sentence_mask'] = attn_mask[0]\n",
    "        \n",
    "        entry['support'] = input_ids[1:]\n",
    "        entry['support_mask'] = attn_mask[1:]\n",
    "        \n",
    "        all_data.append((entry, data['label']))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55328/55328 [00:36<00:00, 1510.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_list = create_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validationset = train_test_split(dataset_list, random_state=2018, test_size=config['test_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset():\n",
    "    with open('model/trainset-' + config['model_save_point'] +'.data', 'wb') as f:\n",
    "        pickle.dump(trainset, f)\n",
    "\n",
    "    with open('model/validationset-' + config['model_save_point'] + '.data', 'wb') as f:\n",
    "        pickle.dump(validationset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(validationset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Our Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "def reset_model():\n",
    "    if model_name == \"distilbert\":\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "                                                \"distilbert-base-uncased\",\n",
    "                                                num_labels = 2, \n",
    "                                                output_attentions = False, \n",
    "                                                output_hidden_states = True, \n",
    "                                            )\n",
    "    elif model_name == \"roberta\":\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "                                                \"roberta-base\",\n",
    "                                                num_labels = 2, \n",
    "                                                output_attentions = True, \n",
    "                                                output_hidden_states = False, \n",
    "                                            )\n",
    "    elif model_name == \"roberta\":\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "                                                \"bert-base-uncased\",\n",
    "                                                num_labels = 2, \n",
    "                                                output_attentions = False, \n",
    "                                                output_hidden_states = False, \n",
    "                                            )\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, (batch, labels) in enumerate(train_loader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
    "\n",
    "        b_input_ids = batch['sentence'].to(device)\n",
    "        b_input_mask = batch['sentence_mask'].to(device)\n",
    "        b_labels = labels.to(device)\n",
    "        model.zero_grad()        \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "                \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)            \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch, b_labels in test_loader:\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(batch['sentence'].to(device), \n",
    "                            attention_mask=batch['sentence_mask'].to(device))\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        prediction = list(np.argmax(logits, axis=1).flatten())\n",
    "        all_predictions.extend(prediction)\n",
    "        all_labels.extend(label_ids.flatten())\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    acc = eval_accuracy/nb_eval_steps\n",
    "    print(\"  Accuracy: {0:.4f}\".format(acc))\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(all_predictions, all_labels, average = 'macro')\n",
    "    precision = precision_score(all_predictions, all_labels, average = 'macro')\n",
    "    recall = recall_score(all_predictions, all_labels, average = 'macro')\n",
    "        \n",
    "    print(\"  F1-score: {0:.4f}\".format(f1))\n",
    "    print(\"  Precision: {0:.4f}\".format(precision))\n",
    "    print(\"  Recall: {0:.4f}\".format(recall))\n",
    "    print()\n",
    "\n",
    "    matrix = confusion_matrix(all_predictions, all_labels)\n",
    "\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    fn = matrix[1][0]\n",
    "    tn = matrix[1][1]\n",
    "    \n",
    "    sarcastic_precision = tp/ (tp + fp)\n",
    "    sarcastic_f1 = 2*tp / (2*tp + fn + fp)\n",
    "    sarcastic_recall = tp / (tp + fn)\n",
    "    \n",
    "    print(\"  Sarcastic Precision: {0:.4f}\".format(sarcastic_precision))\n",
    "    print(\"  Sarcastic F1-score: {0:.4f}\".format(sarcastic_f1))\n",
    "    print(\"  Sarcastic Recall: {0:.4f}\".format(sarcastic_recall)) \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    nonsarcastic_precision = tn / (tn + fn)\n",
    "    nonsarcastic_f1 = 2*tn / (2*tn + fn + fp)\n",
    "    nonsarcastic_recall = tn / (tn + fp)\n",
    "    \n",
    "    print(\"  Non-sarcastic Precision: {0:.4f}\".format(nonsarcastic_precision))\n",
    "    print(\"  Non-Sarcastic F1-score: {0:.4f}\".format(nonsarcastic_f1))\n",
    "    print(\"  Non-sarcasm Recall: {0:.4f}\".format(nonsarcastic_recall))\n",
    "    \n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return [f1, precision, recall, sarcastic_f1, sarcastic_precision, sarcastic_recall, nonsarcastic_f1, nonsarcastic_precision, nonsarcastic_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename = 'model/distilbert-'+ config['model_save_point'] +'.pb'):\n",
    "    print('Saving model...')\n",
    "    torch.save(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, optimizer, scheduler):\n",
    "    loss_values = []\n",
    "    best_results = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    for epoch_i in range(0, config['epochs']):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, config['epochs']))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        train(model, optimizer, scheduler)\n",
    "        results = test(model)\n",
    "        if best_results[0] < results[0]:\n",
    "            best_results = results\n",
    "            save_model()\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:00:07.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:00:08.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:00:09.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:00:11.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:00:12.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:00:13.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:00:14.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:00:16.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:00:17.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:00:18.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:00:19.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:00:21.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:00:22.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:00:23.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:00:25.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:00:26.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:00:27.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:00:28.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:00:30.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:00:31.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:00:32.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:00:33.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:00:35.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:00:36.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:00:37.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:00:38.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:00:40.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:00:41.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:00:42.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:00:44.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:00:45.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:00:46.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:00:47.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:00:49.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:00:50.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:00:51.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:00:52.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:00:54.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:00:55.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:00:56.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:00:57.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:00:59.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:00:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9374\n",
      "  F1-score: 0.9365\n",
      "  Precision: 0.9342\n",
      "  Recall: 0.9402\n",
      "\n",
      "  Sarcastic Precision: 0.9211\n",
      "  Sarcastic F1-score: 0.9441\n",
      "  Sarcastic Recall: 0.9684\n",
      "\n",
      "  Non-sarcastic Precision: 0.9594\n",
      "  Non-Sarcastic F1-score: 0.9288\n",
      "  Non-sarcasm Recall: 0.9001\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:01:12.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:01:14.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:01:15.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:01:16.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:01:17.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:01:19.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:01:20.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:01:21.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:01:22.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:01:24.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:01:25.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:01:26.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:01:28.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:01:29.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:01:30.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:01:31.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:01:33.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:01:34.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:01:35.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:01:36.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:01:38.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:01:39.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:01:40.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:01:41.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:01:43.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:01:44.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:01:45.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:01:46.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:01:48.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:01:49.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:01:50.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:01:52.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:01:53.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:01:54.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:01:55.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:01:57.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:01:58.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:01:59.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:02:00.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:02:02.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:02:03.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:02:04.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:02:05.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:02:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9553\n",
      "  F1-score: 0.9549\n",
      "  Precision: 0.9545\n",
      "  Recall: 0.9553\n",
      "\n",
      "  Sarcastic Precision: 0.9551\n",
      "  Sarcastic F1-score: 0.9593\n",
      "  Sarcastic Recall: 0.9635\n",
      "\n",
      "  Non-sarcastic Precision: 0.9556\n",
      "  Non-Sarcastic F1-score: 0.9505\n",
      "  Non-sarcasm Recall: 0.9454\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:02:19.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:02:20.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:02:22.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:02:23.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:02:24.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:02:25.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:02:27.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:02:28.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:02:29.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:02:32.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:02:33.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:02:34.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:02:35.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:02:36.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:02:38.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:02:39.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:02:40.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:02:41.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:02:43.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:02:44.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:02:45.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:02:46.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:02:49.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:02:50.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:02:51.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:02:53.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:02:54.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:02:55.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:02:56.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:02:58.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:02:59.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:03:00.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:03:01.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:03:03.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:03:04.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:03:05.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:03:06.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:03:08.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:03:09.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:03:10.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:03:11.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9584\n",
      "  F1-score: 0.9580\n",
      "  Precision: 0.9569\n",
      "  Recall: 0.9593\n",
      "\n",
      "  Sarcastic Precision: 0.9518\n",
      "  Sarcastic F1-score: 0.9624\n",
      "  Sarcastic Recall: 0.9731\n",
      "\n",
      "  Non-sarcastic Precision: 0.9668\n",
      "  Non-Sarcastic F1-score: 0.9536\n",
      "  Non-sarcasm Recall: 0.9407\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:03:25.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:03:26.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:03:28.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:03:29.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:03:30.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:03:31.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:03:33.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:03:34.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:03:35.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:03:36.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:03:37.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:03:39.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:03:40.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:03:41.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:03:42.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:03:44.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:03:45.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:03:46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   760  of  1,729.    Elapsed: 0:03:47.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:03:49.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:03:50.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:03:51.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:03:52.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:03:54.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:03:55.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:03:56.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:03:57.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:03:58.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:04:00.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:04:01.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:04:02.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:04:03.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:04:05.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:04:06.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:04:07.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:04:08.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:04:10.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:04:11.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:04:12.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:04:13.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:04:14.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:04:16.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:04:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9622\n",
      "  F1-score: 0.9618\n",
      "  Precision: 0.9620\n",
      "  Recall: 0.9617\n",
      "\n",
      "  Sarcastic Precision: 0.9674\n",
      "  Sarcastic F1-score: 0.9653\n",
      "  Sarcastic Recall: 0.9632\n",
      "\n",
      "  Non-sarcastic Precision: 0.9559\n",
      "  Non-Sarcastic F1-score: 0.9584\n",
      "  Non-sarcasm Recall: 0.9609\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:04:31.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:04:32.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:04:33.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:04:34.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:04:35.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:04:37.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:04:38.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:04:39.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:04:40.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:04:42.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:04:43.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:04:44.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:04:45.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:04:47.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:04:48.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:04:49.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:04:50.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:04:51.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:04:53.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:04:54.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:04:55.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:04:56.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:04:58.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:04:59.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:05:00.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:05:01.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:05:03.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:05:04.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:05:05.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:05:06.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:05:08.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:05:09.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:05:10.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:05:11.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:05:13.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:05:14.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:05:15.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:05:16.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:05:17.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:05:19.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:05:20.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:05:21.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:05:22.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:05:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9622\n",
      "  F1-score: 0.9618\n",
      "  Precision: 0.9612\n",
      "  Recall: 0.9625\n",
      "\n",
      "  Sarcastic Precision: 0.9595\n",
      "  Sarcastic F1-score: 0.9656\n",
      "  Sarcastic Recall: 0.9717\n",
      "\n",
      "  Non-sarcastic Precision: 0.9655\n",
      "  Non-Sarcastic F1-score: 0.9580\n",
      "  Non-sarcasm Recall: 0.9506\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "======== Iteration  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:05:37.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:05:38.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:05:40.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:05:41.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:05:42.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:05:44.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:05:45.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:05:46.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:05:47.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:05:49.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:05:50.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:05:51.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:05:52.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:05:54.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:05:55.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:05:56.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:05:58.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:05:59.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:06:00.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:06:01.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:06:03.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:06:04.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:06:05.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:06:06.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:06:08.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:06:09.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:06:10.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:06:12.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:06:13.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:06:14.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:06:15.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:06:17.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:06:18.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:06:19.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:06:20.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:06:22.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:06:23.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:06:24.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:06:26.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:06:27.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:06:28.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:06:29.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:06:31.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:06:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9387\n",
      "  F1-score: 0.9380\n",
      "  Precision: 0.9369\n",
      "  Recall: 0.9394\n",
      "\n",
      "  Sarcastic Precision: 0.9333\n",
      "  Sarcastic F1-score: 0.9445\n",
      "  Sarcastic Recall: 0.9560\n",
      "\n",
      "  Non-sarcastic Precision: 0.9454\n",
      "  Non-Sarcastic F1-score: 0.9314\n",
      "  Non-sarcasm Recall: 0.9178\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:06:44.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:06:46.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:06:47.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:06:48.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:06:49.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:06:51.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:06:52.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:06:53.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:06:54.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:06:56.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:06:57.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:06:58.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:06:59.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:07:01.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:07:02.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:07:03.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:07:04.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:07:06.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:07:07.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:07:08.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:07:10.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:07:11.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:07:12.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:07:13.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:07:15.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:07:16.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:07:17.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:07:18.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:07:20.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:07:21.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:07:22.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:07:23.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:07:25.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:07:26.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:07:27.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:07:28.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:07:30.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:07:31.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:07:32.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:07:33.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:07:35.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:07:36.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:07:37.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:07:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9550\n",
      "  F1-score: 0.9546\n",
      "  Precision: 0.9545\n",
      "  Recall: 0.9546\n",
      "\n",
      "  Sarcastic Precision: 0.9582\n",
      "  Sarcastic F1-score: 0.9588\n",
      "  Sarcastic Recall: 0.9594\n",
      "\n",
      "  Non-sarcastic Precision: 0.9510\n",
      "  Non-Sarcastic F1-score: 0.9503\n",
      "  Non-sarcasm Recall: 0.9497\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:07:51.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:07:52.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:07:53.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:07:55.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:07:56.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:07:57.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:07:58.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:08:00.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:08:01.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:08:02.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:08:03.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:08:05.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:08:06.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:08:07.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:08:08.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:08:10.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:08:11.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:08:12.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:08:13.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:08:15.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:08:16.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:08:17.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:08:18.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:08:20.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:08:21.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:08:22.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:08:23.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:08:25.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:08:26.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:08:27.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:08:28.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:08:30.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:08:31.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:08:32.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:08:33.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:08:35.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:08:36.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:08:37.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:08:38.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:08:40.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:08:41.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:08:42.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:08:43.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:08:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9582\n",
      "  F1-score: 0.9577\n",
      "  Precision: 0.9567\n",
      "  Recall: 0.9590\n",
      "\n",
      "  Sarcastic Precision: 0.9516\n",
      "  Sarcastic F1-score: 0.9621\n",
      "  Sarcastic Recall: 0.9729\n",
      "\n",
      "  Non-sarcastic Precision: 0.9664\n",
      "  Non-Sarcastic F1-score: 0.9533\n",
      "  Non-sarcasm Recall: 0.9405\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:08:57.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:08:58.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:08:59.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:09:01.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:09:02.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:09:03.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:09:04.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:09:06.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:09:07.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:09:08.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:09:09.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:09:11.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:09:12.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:09:13.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:09:14.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:09:15.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:09:17.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:09:18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   760  of  1,729.    Elapsed: 0:09:19.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:09:20.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:09:22.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:09:23.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:09:24.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:09:25.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:09:27.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:09:28.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:09:29.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:09:30.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:09:32.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:09:33.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:09:34.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:09:35.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:09:36.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:09:38.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:09:39.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:09:40.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:09:41.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:09:43.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:09:44.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:09:45.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:09:46.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:09:48.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:09:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:09:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9610\n",
      "  F1-score: 0.9606\n",
      "  Precision: 0.9601\n",
      "  Recall: 0.9613\n",
      "\n",
      "  Sarcastic Precision: 0.9589\n",
      "  Sarcastic F1-score: 0.9645\n",
      "  Sarcastic Recall: 0.9703\n",
      "\n",
      "  Non-sarcastic Precision: 0.9637\n",
      "  Non-Sarcastic F1-score: 0.9568\n",
      "  Non-sarcasm Recall: 0.9499\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:10:02.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:10:04.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:10:05.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:10:06.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:10:07.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:10:09.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:10:10.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:10:11.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:10:12.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:10:13.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:10:15.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:10:16.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:10:17.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:10:18.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:10:20.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:10:21.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:10:22.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:10:23.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:10:24.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:10:26.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:10:27.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:10:28.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:10:29.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:10:31.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:10:32.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:10:33.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:10:34.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:10:36.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:10:37.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:10:38.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:10:39.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:10:40.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:10:42.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:10:43.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:10:44.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:10:45.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:10:47.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:10:48.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:10:49.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:10:50.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:10:51.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:10:53.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:10:54.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:10:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9614\n",
      "  F1-score: 0.9610\n",
      "  Precision: 0.9604\n",
      "  Recall: 0.9618\n",
      "\n",
      "  Sarcastic Precision: 0.9585\n",
      "  Sarcastic F1-score: 0.9649\n",
      "  Sarcastic Recall: 0.9715\n",
      "\n",
      "  Non-sarcastic Precision: 0.9651\n",
      "  Non-Sarcastic F1-score: 0.9572\n",
      "  Non-sarcasm Recall: 0.9493\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "\n",
      "Training complete!\n",
      "======== Iteration  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:11:10.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:11:11.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:11:12.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:11:13.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:11:15.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:11:16.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:11:17.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:11:18.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:11:20.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:11:21.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:11:22.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:11:24.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:11:25.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:11:26.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:11:27.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:11:29.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:11:30.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:11:31.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:11:33.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:11:34.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:11:35.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:11:36.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:11:38.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:11:39.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:11:40.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:11:41.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:11:43.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:11:44.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:11:45.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:11:47.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:11:48.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:11:49.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:11:50.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:11:52.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:11:53.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:11:54.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:11:55.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:11:57.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:11:58.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:11:59.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:12:01.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:12:02.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:12:03.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:12:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9435\n",
      "  F1-score: 0.9429\n",
      "  Precision: 0.9421\n",
      "  Recall: 0.9438\n",
      "\n",
      "  Sarcastic Precision: 0.9410\n",
      "  Sarcastic F1-score: 0.9487\n",
      "  Sarcastic Recall: 0.9565\n",
      "\n",
      "  Non-sarcastic Precision: 0.9465\n",
      "  Non-Sarcastic F1-score: 0.9371\n",
      "  Non-sarcasm Recall: 0.9278\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:12:17.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:12:18.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:12:19.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:12:21.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:12:22.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:12:23.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:12:24.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:12:26.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:12:27.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:12:28.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:12:29.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:12:31.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:12:32.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:12:33.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:12:34.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:12:36.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:12:37.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:12:38.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:12:39.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:12:41.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:12:42.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:12:43.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:12:44.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:12:46.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:12:47.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:12:48.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:12:50.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:12:51.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:12:52.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:12:53.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:12:55.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:12:56.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:12:57.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:12:58.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:13:00.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:13:01.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:13:02.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:13:03.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:13:05.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:13:06.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:13:07.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:13:08.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:13:10.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:13:10\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9551\n",
      "  F1-score: 0.9546\n",
      "  Precision: 0.9536\n",
      "  Recall: 0.9560\n",
      "\n",
      "  Sarcastic Precision: 0.9489\n",
      "  Sarcastic F1-score: 0.9594\n",
      "  Sarcastic Recall: 0.9702\n",
      "\n",
      "  Non-sarcastic Precision: 0.9631\n",
      "  Non-Sarcastic F1-score: 0.9499\n",
      "  Non-sarcasm Recall: 0.9371\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:13:23.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:13:25.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:13:26.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:13:27.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:13:28.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:13:30.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:13:31.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:13:32.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:13:33.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:13:35.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:13:36.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:13:37.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:13:38.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:13:39.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:13:41.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:13:42.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:13:43.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:13:44.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:13:46.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:13:47.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:13:48.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:13:49.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:13:51.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:13:52.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:13:53.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:13:54.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:13:56.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:13:57.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:13:58.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:13:59.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:14:01.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:14:02.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:14:03.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:14:04.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:14:06.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:14:07.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:14:08.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:14:09.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:14:11.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:14:12.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:14:13.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:14:14.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:14:15.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:14:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9587\n",
      "  F1-score: 0.9583\n",
      "  Precision: 0.9575\n",
      "  Recall: 0.9591\n",
      "\n",
      "  Sarcastic Precision: 0.9549\n",
      "  Sarcastic F1-score: 0.9625\n",
      "  Sarcastic Recall: 0.9702\n",
      "\n",
      "  Non-sarcastic Precision: 0.9634\n",
      "  Non-Sarcastic F1-score: 0.9540\n",
      "  Non-sarcasm Recall: 0.9449\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:14:29.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:14:30.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:14:32.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:14:33.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:14:34.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:14:35.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:14:37.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:14:38.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:14:39.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:14:40.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:14:42.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:14:43.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:14:44.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:14:45.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:14:46.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:14:48.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:14:49.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:14:50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   760  of  1,729.    Elapsed: 0:14:51.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:14:53.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:14:54.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:14:55.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:14:56.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:14:58.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:14:59.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:15:00.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:15:01.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:15:03.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:15:04.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:15:05.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:15:06.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:15:08.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:15:09.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:15:10.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:15:11.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:15:13.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:15:14.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:15:15.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:15:16.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:15:18.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:15:19.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:15:20.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:15:21.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:15:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9612\n",
      "  F1-score: 0.9608\n",
      "  Precision: 0.9601\n",
      "  Recall: 0.9617\n",
      "\n",
      "  Sarcastic Precision: 0.9571\n",
      "  Sarcastic F1-score: 0.9648\n",
      "  Sarcastic Recall: 0.9725\n",
      "\n",
      "  Non-sarcastic Precision: 0.9663\n",
      "  Non-Sarcastic F1-score: 0.9568\n",
      "  Non-sarcasm Recall: 0.9476\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:15:35.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:15:36.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:15:37.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:15:38.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:15:40.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:15:41.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:15:42.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:15:43.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:15:44.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:15:45.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:15:47.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:15:48.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:15:49.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:15:50.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:15:51.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:15:52.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:15:54.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:15:55.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:15:56.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:15:57.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:15:58.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:15:59.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:16:01.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:16:02.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:16:03.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:16:04.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:16:05.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:16:07.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:16:08.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:16:09.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:16:10.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:16:11.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:16:12.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:16:14.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:16:15.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:16:16.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:16:17.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:16:18.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:16:19.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:16:21.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:16:22.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:16:23.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:16:24.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:16:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9624\n",
      "  F1-score: 0.9620\n",
      "  Precision: 0.9615\n",
      "  Recall: 0.9626\n",
      "\n",
      "  Sarcastic Precision: 0.9604\n",
      "  Sarcastic F1-score: 0.9658\n",
      "  Sarcastic Recall: 0.9712\n",
      "\n",
      "  Non-sarcastic Precision: 0.9649\n",
      "  Non-Sarcastic F1-score: 0.9583\n",
      "  Non-sarcasm Recall: 0.9518\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "\n",
      "Training complete!\n",
      "======== Iteration  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:16:40.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:16:41.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:16:42.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:16:44.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:16:45.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:16:46.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:16:47.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:16:49.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:16:50.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:16:51.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:16:52.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:16:54.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:16:55.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:16:56.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:16:58.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:16:59.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:17:00.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:17:01.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:17:03.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:17:04.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:17:05.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:17:06.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:17:08.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:17:09.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:17:10.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:17:12.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:17:13.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:17:14.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:17:15.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:17:17.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:17:18.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:17:19.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:17:20.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:17:22.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:17:23.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:17:24.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:17:26.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:17:27.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:17:28.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:17:29.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:17:31.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:17:32.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:17:33.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:17:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9379\n",
      "  F1-score: 0.9377\n",
      "  Precision: 0.9404\n",
      "  Recall: 0.9369\n",
      "\n",
      "  Sarcastic Precision: 0.9710\n",
      "  Sarcastic F1-score: 0.9414\n",
      "  Sarcastic Recall: 0.9136\n",
      "\n",
      "  Non-sarcastic Precision: 0.9029\n",
      "  Non-Sarcastic F1-score: 0.9339\n",
      "  Non-sarcasm Recall: 0.9672\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:17:47.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:17:48.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:17:49.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:17:51.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:17:52.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:17:53.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:17:54.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:17:56.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:17:57.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:17:58.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:17:59.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:18:01.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:18:02.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:18:03.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:18:04.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:18:06.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:18:07.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:18:08.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:18:09.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:18:11.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:18:12.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:18:13.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:18:15.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:18:16.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:18:17.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:18:18.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:18:20.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:18:21.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:18:22.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:18:23.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:18:25.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:18:26.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:18:27.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:18:28.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:18:30.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:18:31.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:18:32.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:18:33.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:18:35.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:18:36.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:18:37.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:18:38.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:18:40.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:18:40\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9537\n",
      "  F1-score: 0.9533\n",
      "  Precision: 0.9533\n",
      "  Recall: 0.9534\n",
      "\n",
      "  Sarcastic Precision: 0.9570\n",
      "  Sarcastic F1-score: 0.9577\n",
      "  Sarcastic Recall: 0.9584\n",
      "\n",
      "  Non-sarcastic Precision: 0.9498\n",
      "  Non-Sarcastic F1-score: 0.9490\n",
      "  Non-sarcasm Recall: 0.9481\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:18:53.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:18:55.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:18:56.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:18:57.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:18:58.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:19:00.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:19:01.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:19:02.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:19:03.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:19:05.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:19:06.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:19:07.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:19:08.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:19:09.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:19:11.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:19:12.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:19:13.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:19:14.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:19:16.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:19:17.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:19:18.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:19:19.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:19:21.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:19:22.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:19:23.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:19:24.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:19:26.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:19:27.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:19:28.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:19:29.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:19:31.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:19:32.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:19:33.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:19:34.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:19:36.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:19:37.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:19:38.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:19:39.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:19:41.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:19:42.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:19:43.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:19:44.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:19:46.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:19:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9579\n",
      "  F1-score: 0.9575\n",
      "  Precision: 0.9567\n",
      "  Recall: 0.9585\n",
      "\n",
      "  Sarcastic Precision: 0.9534\n",
      "  Sarcastic F1-score: 0.9618\n",
      "  Sarcastic Recall: 0.9704\n",
      "\n",
      "  Non-sarcastic Precision: 0.9636\n",
      "  Non-Sarcastic F1-score: 0.9531\n",
      "  Non-sarcasm Recall: 0.9429\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:19:59.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:20:00.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:20:02.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:20:03.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:20:04.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:20:05.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:20:06.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:20:08.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:20:09.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:20:10.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:20:11.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:20:13.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:20:14.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:20:15.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:20:16.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:20:18.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:20:19.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:20:20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   760  of  1,729.    Elapsed: 0:20:21.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:20:23.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:20:24.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:20:25.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:20:26.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:20:27.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:20:29.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:20:30.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:20:31.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:20:32.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:20:34.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:20:35.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:20:36.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:20:37.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:20:39.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:20:40.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:20:41.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:20:42.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:20:43.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:20:45.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:20:46.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:20:47.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:20:48.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:20:50.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:20:51.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:20:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9623\n",
      "  F1-score: 0.9620\n",
      "  Precision: 0.9620\n",
      "  Recall: 0.9620\n",
      "\n",
      "  Sarcastic Precision: 0.9655\n",
      "  Sarcastic F1-score: 0.9655\n",
      "  Sarcastic Recall: 0.9655\n",
      "\n",
      "  Non-sarcastic Precision: 0.9585\n",
      "  Non-Sarcastic F1-score: 0.9585\n",
      "  Non-sarcasm Recall: 0.9585\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:21:04.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:21:06.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:21:07.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:21:08.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:21:09.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:21:11.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:21:12.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:21:13.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:21:14.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:21:16.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:21:17.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:21:18.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:21:19.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:21:20.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:21:22.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:21:23.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:21:24.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:21:25.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:21:27.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:21:28.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:21:29.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:21:30.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:21:31.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:21:33.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:21:34.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:21:35.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:21:36.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:21:38.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:21:39.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:21:40.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:21:41.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:21:43.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:21:44.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:21:45.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:21:46.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:21:47.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:21:49.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:21:50.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:21:51.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:21:52.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:21:54.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:21:55.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:21:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:21:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9615\n",
      "  F1-score: 0.9612\n",
      "  Precision: 0.9610\n",
      "  Recall: 0.9614\n",
      "\n",
      "  Sarcastic Precision: 0.9633\n",
      "  Sarcastic F1-score: 0.9649\n",
      "  Sarcastic Recall: 0.9664\n",
      "\n",
      "  Non-sarcastic Precision: 0.9594\n",
      "  Non-Sarcastic F1-score: 0.9575\n",
      "  Non-sarcasm Recall: 0.9556\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "======== Iteration  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:22:11.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:22:12.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:22:13.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:22:15.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:22:16.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:22:17.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:22:18.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:22:20.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:22:21.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:22:22.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:22:23.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:22:25.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:22:26.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:22:27.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:22:29.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:22:30.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:22:31.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:22:32.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:22:34.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:22:35.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:22:36.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:22:38.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:22:39.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:22:40.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:22:41.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:22:43.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:22:44.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:22:45.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:22:46.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:22:48.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:22:49.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:22:50.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:22:52.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:22:53.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:22:54.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:22:55.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:22:57.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:22:58.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:22:59.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:23:00.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:23:02.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:23:03.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:23:04.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:23:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9334\n",
      "  F1-score: 0.9325\n",
      "  Precision: 0.9307\n",
      "  Recall: 0.9353\n",
      "\n",
      "  Sarcastic Precision: 0.9216\n",
      "  Sarcastic F1-score: 0.9403\n",
      "  Sarcastic Recall: 0.9598\n",
      "\n",
      "  Non-sarcastic Precision: 0.9490\n",
      "  Non-Sarcastic F1-score: 0.9248\n",
      "  Non-sarcasm Recall: 0.9017\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:23:18.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:23:19.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:23:20.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:23:22.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:23:23.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:23:24.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:23:25.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:23:27.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:23:28.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:23:29.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:23:30.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:23:32.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:23:33.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:23:34.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:23:35.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:23:37.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:23:38.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:23:39.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:23:40.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:23:42.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:23:43.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:23:44.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:23:45.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:23:47.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:23:48.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:23:49.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:23:51.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:23:52.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:23:53.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:23:54.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:23:56.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:23:57.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:23:58.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:23:59.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:24:01.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:24:02.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:24:03.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:24:04.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:24:06.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:24:07.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:24:08.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:24:09.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:24:11.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:24:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9548\n",
      "  F1-score: 0.9543\n",
      "  Precision: 0.9532\n",
      "  Recall: 0.9557\n",
      "\n",
      "  Sarcastic Precision: 0.9479\n",
      "  Sarcastic F1-score: 0.9591\n",
      "  Sarcastic Recall: 0.9706\n",
      "\n",
      "  Non-sarcastic Precision: 0.9635\n",
      "  Non-Sarcastic F1-score: 0.9494\n",
      "  Non-sarcasm Recall: 0.9358\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:24:24.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:24:25.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:24:27.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:24:28.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:24:29.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:24:30.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:24:32.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:24:33.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:24:34.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:24:35.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:24:37.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:24:38.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:24:39.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:24:40.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:24:42.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:24:43.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:24:44.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:24:45.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:24:47.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:24:48.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:24:49.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:24:50.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:24:52.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:24:53.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:24:54.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:24:55.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:24:57.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:24:58.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:24:59.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:25:00.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:25:01.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:25:03.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:25:04.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:25:05.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:25:06.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:25:08.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:25:09.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:25:10.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:25:11.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:25:13.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:25:14.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:25:15.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:25:16.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:25:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9548\n",
      "  F1-score: 0.9542\n",
      "  Precision: 0.9525\n",
      "  Recall: 0.9567\n",
      "\n",
      "  Sarcastic Precision: 0.9422\n",
      "  Sarcastic F1-score: 0.9594\n",
      "  Sarcastic Recall: 0.9771\n",
      "\n",
      "  Non-sarcastic Precision: 0.9712\n",
      "  Non-Sarcastic F1-score: 0.9490\n",
      "  Non-sarcasm Recall: 0.9279\n",
      "  Validation took: 0:00:11\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:25:29.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:25:30.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:25:32.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:25:33.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:25:34.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:25:35.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:25:36.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:25:38.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:25:39.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:25:40.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:25:41.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:25:43.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:25:44.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:25:45.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:25:46.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:25:48.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:25:49.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:25:50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   760  of  1,729.    Elapsed: 0:25:51.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:25:52.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:25:54.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:25:55.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:25:56.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:25:57.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:25:59.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:26:00.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:26:01.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:26:02.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:26:04.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:26:05.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:26:06.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:26:07.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:26:08.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:26:10.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:26:11.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:26:12.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:26:13.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:26:15.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:26:16.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:26:17.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:26:18.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:26:20.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:26:21.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:26:21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9600\n",
      "  F1-score: 0.9596\n",
      "  Precision: 0.9592\n",
      "  Recall: 0.9601\n",
      "\n",
      "  Sarcastic Precision: 0.9592\n",
      "  Sarcastic F1-score: 0.9636\n",
      "  Sarcastic Recall: 0.9680\n",
      "\n",
      "  Non-sarcastic Precision: 0.9610\n",
      "  Non-Sarcastic F1-score: 0.9557\n",
      "  Non-sarcasm Recall: 0.9505\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  1,729.    Elapsed: 0:26:34.\n",
      "  Batch    80  of  1,729.    Elapsed: 0:26:36.\n",
      "  Batch   120  of  1,729.    Elapsed: 0:26:37.\n",
      "  Batch   160  of  1,729.    Elapsed: 0:26:38.\n",
      "  Batch   200  of  1,729.    Elapsed: 0:26:39.\n",
      "  Batch   240  of  1,729.    Elapsed: 0:26:40.\n",
      "  Batch   280  of  1,729.    Elapsed: 0:26:42.\n",
      "  Batch   320  of  1,729.    Elapsed: 0:26:43.\n",
      "  Batch   360  of  1,729.    Elapsed: 0:26:44.\n",
      "  Batch   400  of  1,729.    Elapsed: 0:26:45.\n",
      "  Batch   440  of  1,729.    Elapsed: 0:26:47.\n",
      "  Batch   480  of  1,729.    Elapsed: 0:26:48.\n",
      "  Batch   520  of  1,729.    Elapsed: 0:26:49.\n",
      "  Batch   560  of  1,729.    Elapsed: 0:26:50.\n",
      "  Batch   600  of  1,729.    Elapsed: 0:26:51.\n",
      "  Batch   640  of  1,729.    Elapsed: 0:26:53.\n",
      "  Batch   680  of  1,729.    Elapsed: 0:26:54.\n",
      "  Batch   720  of  1,729.    Elapsed: 0:26:55.\n",
      "  Batch   760  of  1,729.    Elapsed: 0:26:56.\n",
      "  Batch   800  of  1,729.    Elapsed: 0:26:58.\n",
      "  Batch   840  of  1,729.    Elapsed: 0:26:59.\n",
      "  Batch   880  of  1,729.    Elapsed: 0:27:00.\n",
      "  Batch   920  of  1,729.    Elapsed: 0:27:01.\n",
      "  Batch   960  of  1,729.    Elapsed: 0:27:02.\n",
      "  Batch 1,000  of  1,729.    Elapsed: 0:27:04.\n",
      "  Batch 1,040  of  1,729.    Elapsed: 0:27:05.\n",
      "  Batch 1,080  of  1,729.    Elapsed: 0:27:06.\n",
      "  Batch 1,120  of  1,729.    Elapsed: 0:27:07.\n",
      "  Batch 1,160  of  1,729.    Elapsed: 0:27:09.\n",
      "  Batch 1,200  of  1,729.    Elapsed: 0:27:10.\n",
      "  Batch 1,240  of  1,729.    Elapsed: 0:27:11.\n",
      "  Batch 1,280  of  1,729.    Elapsed: 0:27:12.\n",
      "  Batch 1,320  of  1,729.    Elapsed: 0:27:14.\n",
      "  Batch 1,360  of  1,729.    Elapsed: 0:27:15.\n",
      "  Batch 1,400  of  1,729.    Elapsed: 0:27:16.\n",
      "  Batch 1,440  of  1,729.    Elapsed: 0:27:17.\n",
      "  Batch 1,480  of  1,729.    Elapsed: 0:27:18.\n",
      "  Batch 1,520  of  1,729.    Elapsed: 0:27:20.\n",
      "  Batch 1,560  of  1,729.    Elapsed: 0:27:21.\n",
      "  Batch 1,600  of  1,729.    Elapsed: 0:27:22.\n",
      "  Batch 1,640  of  1,729.    Elapsed: 0:27:23.\n",
      "  Batch 1,680  of  1,729.    Elapsed: 0:27:25.\n",
      "  Batch 1,720  of  1,729.    Elapsed: 0:27:26.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:27:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9618\n",
      "  F1-score: 0.9614\n",
      "  Precision: 0.9610\n",
      "  Recall: 0.9619\n",
      "\n",
      "  Sarcastic Precision: 0.9608\n",
      "  Sarcastic F1-score: 0.9652\n",
      "  Sarcastic Recall: 0.9696\n",
      "\n",
      "  Non-sarcastic Precision: 0.9630\n",
      "  Non-Sarcastic F1-score: 0.9577\n",
      "  Non-sarcasm Recall: 0.9524\n",
      "  Validation took: 0:00:11\n",
      "Saving model...\n",
      "\n",
      "Training complete!\n",
      "  F1-score: 0.9616\n",
      "  Precision: 0.9613\n",
      "  Recall: 0.9620\n",
      "\n",
      "  Sarcastic Precision: 0.9653\n",
      "  Sarcastic F1-score: 0.9621\n",
      "  Sarcastic Recall: 0.9686\n",
      "\n",
      "  Non-sarcastic Precision: 0.9579\n",
      "  Non-Sarcastic F1-score: 0.9620\n",
      "  Non-sarcasm Recall: 0.9540\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "total_iterations = 5\n",
    "for iteration in range(total_iterations):\n",
    "    print(\"======== Iteration {:2d}\".format(iteration))\n",
    "    model, optimizer, scheduler = reset_model()\n",
    "    result = run(model, optimizer, scheduler)\n",
    "    all_results.append(result)\n",
    "\n",
    "final_results = [sum(value)/3 for value in zip(all_results[0], all_results[1], all_results[2])]\n",
    "\n",
    "print(\"  F1-score: {0:.4f}\".format(final_results[0]))\n",
    "print(\"  Precision: {0:.4f}\".format(final_results[1]))\n",
    "print(\"  Recall: {0:.4f}\".format(final_results[2]))\n",
    "print()\n",
    "\n",
    "print(\"  Sarcastic Precision: {0:.4f}\".format(final_results[3]))\n",
    "print(\"  Sarcastic F1-score: {0:.4f}\".format(final_results[4]))\n",
    "print(\"  Sarcastic Recall: {0:.4f}\".format(final_results[5])) \n",
    "\n",
    "print()\n",
    "\n",
    "print(\"  Non-sarcastic Precision: {0:.4f}\".format(final_results[6]))\n",
    "print(\"  Non-Sarcastic F1-score: {0:.4f}\".format(final_results[7]))\n",
    "print(\"  Non-sarcasm Recall: {0:.4f}\".format(final_results[8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
